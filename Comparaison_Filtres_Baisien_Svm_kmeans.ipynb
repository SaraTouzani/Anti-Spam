{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "import os\n",
    "import codecs\n",
    "import re\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import numpy\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from pandas import DataFrame\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn import svm\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "NEWLINE = '\\n'\n",
    "SKIP_FILES = {'cmds'}\n",
    "\n",
    "#Text Processing\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "#Text Processing\n",
    "def preprocess(raw) :\n",
    "    wordlist = word_tokenize(raw)\n",
    "    returnlist = []\n",
    "    for word in wordlist :\n",
    "        if (word not in stopwords.words(\"english\")) :\n",
    "            if (word[0] in (('1','2','3','4','5','6','7','8','9','0','&','(','-','_','ç','é','à',')','=','+','°','~','#'))) :\n",
    "                returnlist.append('#')\n",
    "            else :\n",
    "                returnlist.append(stemmer.stem(word))\n",
    "    return returnlist\n",
    "\n",
    "def read_files(path):\n",
    "    for root, dir_names, file_names in os.walk(path):\n",
    "        for path in dir_names:\n",
    "            read_files(os.path.join(root, path))\n",
    "        for file_name in file_names:\n",
    "            if file_name not in SKIP_FILES:\n",
    "                file_path = os.path.join(root, file_name)\n",
    "                if os.path.isfile(file_path):\n",
    "                    lines = []\n",
    "                    f = open(file_path, encoding=\"latin-1\")\n",
    "                    for line in f:\n",
    "                        lines.append(line)\n",
    "\n",
    "                    f.close()\n",
    "                    content = NEWLINE.join(lines)\n",
    "                    yield file_path, content\n",
    "\n",
    "def build_data_frame(path, classification):\n",
    "    rows = []\n",
    "    index = []\n",
    "    for file_name, text in read_files(path):\n",
    "        rows.append({'text': text, 'class': classification})\n",
    "        index.append(file_name)\n",
    "\n",
    "    data_frame = DataFrame(rows, index=index)\n",
    "    return data_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing de Données"
   ]
  },
  
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>/home/toutou/Téléchargements/data/enron1/ham/0638.2000-03-20.farmer.ham.txt</th>\n",
       "      <td>ham</td>\n",
       "      <td>Subject: @ ect . enron . com email notificatio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>/home/toutou/Téléchargements/data/enron1/ham/2506.2000-10-10.farmer.ham.txt</th>\n",
       "      <td>ham</td>\n",
       "      <td>Subject: union carbide - seadrift\\n\\ndaren\\n\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>/home/toutou/Téléchargements/data/enron1/ham/0479.2000-02-24.farmer.ham.txt</th>\n",
       "      <td>ham</td>\n",
       "      <td>Subject: midcon invoices\\n\\nkellie -\\n\\ni rese...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>/home/toutou/Téléchargements/data/enron1/spam/2285.2004-09-26.GP.spam.txt</th>\n",
       "      <td>spam</td>\n",
       "      <td>Subject: fw : hungry 30 to 40 girls wants to d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>/home/toutou/Téléchargements/data/enron1/ham/5063.2001-11-12.farmer.ham.txt</th>\n",
       "      <td>ham</td>\n",
       "      <td>Subject: pipelines that still have dial in acc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>/home/toutou/Téléchargements/data/enron1/ham/2925.2000-11-22.farmer.ham.txt</th>\n",
       "      <td>ham</td>\n",
       "      <td>Subject: re : cornhusker\\n\\nthanks for the inf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>/home/toutou/Téléchargements/data/enron1/ham/0907.2000-04-11.farmer.ham.txt</th>\n",
       "      <td>ham</td>\n",
       "      <td>Subject: re : fyi - wellhead portfolio\\n\\nwho ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>/home/toutou/Téléchargements/data/enron1/spam/4922.2005-07-25.GP.spam.txt</th>\n",
       "      <td>spam</td>\n",
       "      <td>Subject: u . s . robotics - analogue / wired c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>/home/toutou/Téléchargements/data/enron1/spam/4478.2005-05-12.GP.spam.txt</th>\n",
       "      <td>spam</td>\n",
       "      <td>Subject: fda approved\\n\\nwe are one of the top...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>/home/toutou/Téléchargements/data/enron1/ham/0188.2000-01-12.farmer.ham.txt</th>\n",
       "      <td>ham</td>\n",
       "      <td>Subject: re : hl &amp; p for 12 / 99\\n\\nit is the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>/home/toutou/Téléchargements/data/enron1/spam/4463.2005-05-08.GP.spam.txt</th>\n",
       "      <td>spam</td>\n",
       "      <td>Subject: highest gains without guesswork\\n\\nwy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>/home/toutou/Téléchargements/data/enron1/ham/2584.2000-10-18.farmer.ham.txt</th>\n",
       "      <td>ham</td>\n",
       "      <td>Subject: tenaska iv gas management agreement (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>/home/toutou/Téléchargements/data/enron1/ham/3984.2001-03-23.farmer.ham.txt</th>\n",
       "      <td>ham</td>\n",
       "      <td>Subject: phillips - 4 / 01\\n\\ncarlos ,\\n\\ni cr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>/home/toutou/Téléchargements/data/enron1/ham/0783.2000-03-29.farmer.ham.txt</th>\n",
       "      <td>ham</td>\n",
       "      <td>Subject: re : resume attached\\n\\nliz is an mit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>/home/toutou/Téléchargements/data/enron1/ham/2501.2000-10-10.farmer.ham.txt</th>\n",
       "      <td>ham</td>\n",
       "      <td>Subject: rate for tenaska deal\\n\\ndaren ,\\n\\nw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>/home/toutou/Téléchargements/data/enron1/ham/0527.2000-03-01.farmer.ham.txt</th>\n",
       "      <td>ham</td>\n",
       "      <td>Subject: organizational announcement\\n\\nplease...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>/home/toutou/Téléchargements/data/enron1/spam/4665.2005-06-09.GP.spam.txt</th>\n",
       "      <td>spam</td>\n",
       "      <td>Subject: a more radiant you\\n\\nact now . get y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>/home/toutou/Téléchargements/data/enron1/spam/4486.2005-05-15.GP.spam.txt</th>\n",
       "      <td>spam</td>\n",
       "      <td>Subject: all mens need this 5 j\\n\\nci - ialis ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>/home/toutou/Téléchargements/data/enron1/ham/2317.2000-09-22.farmer.ham.txt</th>\n",
       "      <td>ham</td>\n",
       "      <td>Subject: revised : eastrans nomination change ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>/home/toutou/Téléchargements/data/enron1/spam/0779.2004-04-05.GP.spam.txt</th>\n",
       "      <td>spam</td>\n",
       "      <td>Subject: cheap v . iagra , phentermine , xa . ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>/home/toutou/Téléchargements/data/enron1/ham/1776.2000-07-27.farmer.ham.txt</th>\n",
       "      <td>ham</td>\n",
       "      <td>Subject: enron / hpl actuals for july 26 , 200...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>/home/toutou/Téléchargements/data/enron1/ham/4436.2001-04-30.farmer.ham.txt</th>\n",
       "      <td>ham</td>\n",
       "      <td>Subject: conoco , inc . katy tailgate contract...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>/home/toutou/Téléchargements/data/enron1/ham/3413.2001-01-24.farmer.ham.txt</th>\n",
       "      <td>ham</td>\n",
       "      <td>Subject: eastrans nomination effective 1 / 25 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>/home/toutou/Téléchargements/data/enron1/spam/4320.2005-04-20.GP.spam.txt</th>\n",
       "      <td>spam</td>\n",
       "      <td>Subject: give a man a fish and he will eat for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>/home/toutou/Téléchargements/data/enron1/ham/2357.2000-09-27.farmer.ham.txt</th>\n",
       "      <td>ham</td>\n",
       "      <td>Subject: accum .\\n\\n- - - - - - - - - - - - - ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>/home/toutou/Téléchargements/data/enron1/spam/0686.2004-03-20.GP.spam.txt</th>\n",
       "      <td>spam</td>\n",
       "      <td>Subject: fwd : real buy v / a / lium ) xan @ x...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>/home/toutou/Téléchargements/data/enron1/spam/4377.2005-04-25.GP.spam.txt</th>\n",
       "      <td>spam</td>\n",
       "      <td>Subject: stock market standouts\\n\\ninfotex hol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>/home/toutou/Téléchargements/data/enron1/spam/0289.2004-01-23.GP.spam.txt</th>\n",
       "      <td>spam</td>\n",
       "      <td>Subject: paliourg get the doctors time 4 freee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>/home/toutou/Téléchargements/data/enron1/ham/4263.2001-04-10.farmer.ham.txt</th>\n",
       "      <td>ham</td>\n",
       "      <td>Subject: re : noms / actual flow for 4 / 09 / ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>/home/toutou/Téléchargements/data/enron1/ham/0196.2000-01-13.farmer.ham.txt</th>\n",
       "      <td>ham</td>\n",
       "      <td>Subject: entex estimates for 12 / 99\\n\\nattach...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>/home/toutou/Téléchargements/data/enron1/ham/0570.2000-03-07.farmer.ham.txt</th>\n",
       "      <td>ham</td>\n",
       "      <td>Subject: eops salary survey questionnaire\\n\\np...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>/home/toutou/Téléchargements/data/enron1/spam/5020.2005-08-16.GP.spam.txt</th>\n",
       "      <td>spam</td>\n",
       "      <td>Subject: funny\\n\\nmay aplomb be angola may pri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>/home/toutou/Téléchargements/data/enron1/ham/1153.2000-05-26.farmer.ham.txt</th>\n",
       "      <td>ham</td>\n",
       "      <td>Subject: nomination 6 / 1 / 2000 - eastrans\\n\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>/home/toutou/Téléchargements/data/enron1/ham/2246.2000-09-15.farmer.ham.txt</th>\n",
       "      <td>ham</td>\n",
       "      <td>Subject: the enrononline games\\n\\n- - - - - - ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>/home/toutou/Téléchargements/data/enron1/ham/3595.2001-02-16.farmer.ham.txt</th>\n",
       "      <td>ham</td>\n",
       "      <td>Subject: re : first delivery - helmerich &amp; pay...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>/home/toutou/Téléchargements/data/enron1/ham/3135.2000-12-19.farmer.ham.txt</th>\n",
       "      <td>ham</td>\n",
       "      <td>Subject: tenaska iv pricing\\n\\ni think we need...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>/home/toutou/Téléchargements/data/enron1/spam/4615.2005-06-02.GP.spam.txt</th>\n",
       "      <td>spam</td>\n",
       "      <td>Subject: re : keeping it like a rock\\n\\nhello ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>/home/toutou/Téléchargements/data/enron1/ham/2977.2000-11-30.farmer.ham.txt</th>\n",
       "      <td>ham</td>\n",
       "      <td>Subject: sitara / cpr availability\\n\\nto all s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>/home/toutou/Téléchargements/data/enron1/ham/0275.2000-01-28.farmer.ham.txt</th>\n",
       "      <td>ham</td>\n",
       "      <td>Subject: follow - up\\n\\njust following up to m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>/home/toutou/Téléchargements/data/enron1/spam/4862.2005-07-11.GP.spam.txt</th>\n",
       "      <td>spam</td>\n",
       "      <td>Subject: check the superb specials on top - se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>/home/toutou/Téléchargements/data/enron1/ham/3758.2001-03-12.farmer.ham.txt</th>\n",
       "      <td>ham</td>\n",
       "      <td>Subject: registration welcome email\\n\\nthank y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>/home/toutou/Téléchargements/data/enron1/spam/4720.2005-06-16.GP.spam.txt</th>\n",
       "      <td>spam</td>\n",
       "      <td>Subject: want something extra in bed ?\\n\\nhi t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>/home/toutou/Téléchargements/data/enron1/ham/5102.2001-12-08.farmer.ham.txt</th>\n",
       "      <td>ham</td>\n",
       "      <td>Subject: neon retreat\\n\\nho ho ho , we ' re ar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>/home/toutou/Téléchargements/data/enron1/spam/3319.2004-12-27.GP.spam.txt</th>\n",
       "      <td>spam</td>\n",
       "      <td>Subject: over 160 fda approved meds\\n\\n% rnd _...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>/home/toutou/Téléchargements/data/enron1/spam/3009.2004-12-01.GP.spam.txt</th>\n",
       "      <td>spam</td>\n",
       "      <td>Subject: have the ability to attract members o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>/home/toutou/Téléchargements/data/enron1/ham/4903.2001-09-14.farmer.ham.txt</th>\n",
       "      <td>ham</td>\n",
       "      <td>Subject: fw : black marlin\\n\\nhave you had a c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>/home/toutou/Téléchargements/data/enron1/spam/3963.2005-03-03.GP.spam.txt</th>\n",
       "      <td>spam</td>\n",
       "      <td>Subject: you need this abazis\\n\\nlook at this ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>/home/toutou/Téléchargements/data/enron1/ham/1977.2000-08-18.farmer.ham.txt</th>\n",
       "      <td>ham</td>\n",
       "      <td>Subject: unify passwords will be reset under s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>/home/toutou/Téléchargements/data/enron1/spam/3789.2005-02-10.GP.spam.txt</th>\n",
       "      <td>spam</td>\n",
       "      <td>Subject: looking for a new date\\n\\nfind a new ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>/home/toutou/Téléchargements/data/enron1/ham/4614.2001-05-24.farmer.ham.txt</th>\n",
       "      <td>ham</td>\n",
       "      <td>Subject: eastrans nomination for 6 / 01 / 01\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>/home/toutou/Téléchargements/data/enron1/ham/1144.2000-05-25.farmer.ham.txt</th>\n",
       "      <td>ham</td>\n",
       "      <td>Subject: june 2000 co - owners volumes\\n\\n- - ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>/home/toutou/Téléchargements/data/enron1/spam/2829.2004-11-16.GP.spam.txt</th>\n",
       "      <td>spam</td>\n",
       "      <td>Subject: approval # 5146\\n\\nhello ,\\n\\nwe sent...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>/home/toutou/Téléchargements/data/enron1/ham/0384.2000-02-09.farmer.ham.txt</th>\n",
       "      <td>ham</td>\n",
       "      <td>Subject: pathing procedures for buybacks\\n\\nto...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>/home/toutou/Téléchargements/data/enron1/spam/4647.2005-06-05.GP.spam.txt</th>\n",
       "      <td>spam</td>\n",
       "      <td>Subject: copy from cassette tape to mp 3 and v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>/home/toutou/Téléchargements/data/enron1/spam/3845.2005-02-15.GP.spam.txt</th>\n",
       "      <td>spam</td>\n",
       "      <td>Subject: 108 mbps wireless firewall 4 - port r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>/home/toutou/Téléchargements/data/enron1/spam/2526.2004-10-17.GP.spam.txt</th>\n",
       "      <td>spam</td>\n",
       "      <td>Subject: your contact info\\n\\n? \" ? ? ?  ?  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>/home/toutou/Téléchargements/data/enron1/ham/0624.2000-03-19.farmer.ham.txt</th>\n",
       "      <td>ham</td>\n",
       "      <td>Subject: team room\\n\\n- - - - - - - - - - - - ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>/home/toutou/Téléchargements/data/enron1/ham/3524.2001-02-05.farmer.ham.txt</th>\n",
       "      <td>ham</td>\n",
       "      <td>Subject: eastrans nominations change effective...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>/home/toutou/Téléchargements/data/enron1/ham/3832.2001-03-16.farmer.ham.txt</th>\n",
       "      <td>ham</td>\n",
       "      <td>Subject: re : noms / actual flow for 03 / 15\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>/home/toutou/Téléchargements/data/enron1/ham/4800.2001-08-09.farmer.ham.txt</th>\n",
       "      <td>ham</td>\n",
       "      <td>Subject: fw : executed agency - ena and tenask...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5172 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   class  \\\n",
       "/home/toutou/Téléchargements/data/enron1/ham/06...   ham   \n",
       "/home/toutou/Téléchargements/data/enron1/ham/25...   ham   \n",
       "/home/toutou/Téléchargements/data/enron1/ham/04...   ham   \n",
       "/home/toutou/Téléchargements/data/enron1/spam/2...  spam   \n",
       "/home/toutou/Téléchargements/data/enron1/ham/50...   ham   \n",
       "/home/toutou/Téléchargements/data/enron1/ham/29...   ham   \n",
       "/home/toutou/Téléchargements/data/enron1/ham/09...   ham   \n",
       "/home/toutou/Téléchargements/data/enron1/spam/4...  spam   \n",
       "/home/toutou/Téléchargements/data/enron1/ham/19...   ham   \n",
       "/home/toutou/Téléchargements/data/enron1/spam/3...  spam   \n",
       "/home/toutou/Téléchargements/data/enron1/ham/46...   ham   \n",
       "/home/toutou/Téléchargements/data/enron1/ham/11...   ham   \n",
       "/home/toutou/Téléchargements/data/enron1/spam/2...  spam   \n",
       "/home/toutou/Téléchargements/data/enron1/ham/03...   ham   \n",
       "/home/toutou/Téléchargements/data/enron1/spam/4...  spam   \n",
       "/home/toutou/Téléchargements/data/enron1/spam/3...  spam   \n",
       "/home/toutou/Téléchargements/data/enron1/spam/2...  spam   \n",
       "/home/toutou/Téléchargements/data/enron1/ham/06...   ham   \n",
       "/home/toutou/Téléchargements/data/enron1/ham/35...   ham   \n",
       "/home/toutou/Téléchargements/data/enron1/ham/38...   ham   \n",
       "/home/toutou/Téléchargements/data/enron1/ham/48...   ham   \n",
       "\n",
       "                                                                                                 text  \n",
       "/home/toutou/Téléchargements/data/enron1/ham/06...  Subject: @ ect . enron . com email notificatio...  \n",
       "/home/toutou/Téléchargements/data/enron1/ham/25...  Subject: union carbide - seadrift\\n\\ndaren\\n\\n...  \n",
       "/home/toutou/Téléchargements/data/enron1/ham/04...  Subject: midcon invoices\\n\\nkellie -\\n\\ni rese...  \n",
       "/home/toutou/Téléchargements/data/enron1/spam/2...  Subject: fw : hungry 30 to 40 girls wants to d...  \n",
       "/home/toutou/Téléchargements/data/enron1/ham/50...  Subject: pipelines that still have dial in acc...  \n",
       "/home/toutou/Téléchargements/data/enron1/ham/29...  Subject: re : cornhusker\\n\\nthanks for the inf...  \n",
       "/home/toutou/Téléchargements/data/enron1/ham/09...  Subject: re : fyi - wellhead portfolio\\n\\nwho ...  \n",
       "/home/toutou/Téléchargements/data/enron1/spam/4...  Subject: u . s . robotics - analogue / wired c...  \n",
       "/home/toutou/Téléchargements/data/enron1/spam/4...  Subject: fda approved\\n\\nwe are one of the top...  \n",
       "/home/toutou/Téléchargements/data/enron1/ham/01...  Subject: re : hl & p for 12 / 99\\n\\nit is the ...  \n",
       "/home/toutou/Téléchargements/data/enron1/spam/4...  Subject: highest gains without guesswork\\n\\nwy...  \n",
       "/home/toutou/Téléchargements/data/enron1/ham/25...  Subject: tenaska iv gas management agreement (...  \n",
       "/home/toutou/Téléchargements/data/enron1/ham/42...  Subject: re : noms / actual flow for 4 / 09 / ...  \n",
       "/home/toutou/Téléchargements/data/enron1/ham/01...  Subject: entex estimates for 12 / 99\\n\\nattach...  \n",
       "...                                                                                               ...  \n",
       "/home/toutou/Téléchargements/data/enron1/ham/05...  Subject: eops salary survey questionnaire\\n\\np...  \n",
       "/home/toutou/Téléchargements/data/enron1/spam/5...  Subject: funny\\n\\nmay aplomb be angola may pri...  \n",
       "/home/toutou/Téléchargements/data/enron1/ham/11...  Subject: nomination 6 / 1 / 2000 - eastrans\\n\\...  \n",
       "/home/toutou/Téléchargements/data/enron1/ham/22...  Subject: the enrononline games\\n\\n- - - - - - ...  \n",
       "/home/toutou/Téléchargements/data/enron1/ham/11...  Subject: june 2000 co - owners volumes\\n\\n- - ...  \n",
       "/home/toutou/Téléchargements/data/enron1/spam/2...  Subject: approval # 5146\\n\\nhello ,\\n\\nwe sent...  \n",
       "/home/toutou/Téléchargements/data/enron1/ham/03...  Subject: pathing procedures for buybacks\\n\\nto...  \n",
       "/home/toutou/Téléchargements/data/enron1/spam/4...  Subject: copy from cassette tape to mp 3 and v...  \n",
       "/home/toutou/Téléchargements/data/enron1/spam/3...  Subject: 108 mbps wireless firewall 4 - port r...  \n",
       "/home/toutou/Téléchargements/data/enron1/spam/2...  Subject: your contact info\\n\\n? \" ? ? ?  ?  ...  \n",
       "/home/toutou/Téléchargements/data/enron1/ham/06...  Subject: team room\\n\\n- - - - - - - - - - - - ...  \n",
       "/home/toutou/Téléchargements/data/enron1/ham/35...  Subject: eastrans nominations change effective...  \n",
       "/home/toutou/Téléchargements/data/enron1/ham/38...  Subject: re : noms / actual flow for 03 / 15\\n...  \n",
       "/home/toutou/Téléchargements/data/enron1/ham/48...  Subject: fw : executed agency - ena and tenask...  \n",
       "\n",
       "[5172 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HAM = 'ham'\n",
    "SPAM = 'spam'\n",
    "\"\"\",\n",
    "    ('/home/techmen/Desktop/IA/dataset/enron2/spam',    SPAM),\n",
    "    ('/home/techmen/Desktop/IA/dataset/enron3/spam',    SPAM),\n",
    "    ('/home/techmen/Desktop/IA/dataset/enron4/spam',      SPAM),\n",
    "    ('/home/techmen/Desktop/IA/dataset/enron5/spam',    SPAM),\n",
    "    ('/home/techmen/Desktop/IA/dataset/enron6/spam',  SPAM),\n",
    "    ('/home/techmen/Desktop/IA/dataset/enron1/ham',        HAM),\n",
    "    ('/home/techmen/Desktop/IA/dataset/enron2/ham',    HAM),\n",
    "    ('/home/techmen/Desktop/IA/dataset/enron3/ham',    HAM),\n",
    "    ('/home/techmen/Desktop/IA/dataset/enron4/ham',      HAM),\n",
    "    ('/home/techmen/Desktop/IA/dataset/enron5/ham',    HAM),\n",
    "    ('/home/techmen/Desktop/IA/dataset/enron6/ham',  HAM),\"\"\"\n",
    "SOURCES = [\n",
    "    ('/home/toutou/Téléchargements/data/enron1/spam',        SPAM),\n",
    "    ('/home/toutou/Téléchargements/data/enron1/ham',        HAM)\n",
    "]\n",
    "\n",
    "data = DataFrame({'text': [], 'class': []})\n",
    "for path, classification in SOURCES:\n",
    "    data = data.append(build_data_frame(path, classification))\n",
    "\n",
    "data = data.reindex(numpy.random.permutation(data.index))\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp=[]\n",
    "hm=[]\n",
    "liste=[]\n",
    "for i in range(len(data['class'])):\n",
    "    if data['class'][i]=='spam':\n",
    "        liste=preprocess(data['text'][i])\n",
    "        sp.extend(liste)\n",
    "    else:\n",
    "        liste=preprocess(data['text'][i])\n",
    "        hm.extend(liste)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Subject: @ ect . enron . com email notification !\\n\\nwe are one @ enron . com !\\n\\nplease be aware of the following senders were automatically notified to ( a ) .\\n\\nstop sending internet mail to your @ ect . enron . com address and to ( b ) . send\\n\\nfuture internet communications to daren . j . farmer @ enron . com :\\n\\nfpam _ @ hotmail . com , mjones 7 @ txu . com\\n\\nreminder :\\n\\nyour @ ect . enron . com address should not be used any longer and will be\\n\\ndeactivated soon . so please make sure these contacts switch to your new\\n\\n@ enron . com address . if you have subscribed to mailing lists , please make\\n\\nsure to update your addresses there as well .\\n\\nand\\n\\nyour shortname @ enron . com address ( i . e . jsmith @ enron . com ) will continue to\\n\\nwork , even though your formal address is longname @ enron . com ( i . e .\\n\\njohn . smith @ enron . com )\\n\\nplease do not reply to this message as it was automatically generated .'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ham     3672\n",
       "spam    1500\n",
       "Name: class, dtype: int64"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['text'].values\n",
    "worddata =[word_tokenize(c) for c in data['text'].values ]\n",
    "data['class'].value_counts()\n",
    "#data['text'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},

     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#garder queles elments distinctes dans les deux listes\n",
    "hm = list(set(hm))\n",
    "sp = list(set(sp))\n",
    "\n",
    "#matrice contenant la frequence d'apparution de chaque mot \n",
    "hm_t=[[x,0] for x in hm]\n",
    "sp_t=[[x,0] for x in sp]\n",
    "\n",
    "sp = list(set(sp))\n",
    "for j in range(1000): #len(hm)\n",
    "    for i in range(len(data['class'])):  #len(data['class'])-5000\n",
    "        if data['class'][i]=='ham':\n",
    "            hm_t[j][1]+=data['text'][i].count(hm[j])\n",
    "            \n",
    "            \n",
    "for j in range(1000): #len(sp)\n",
    "    for i in range(len(data['class'])):  #len(data['class'])-5000\n",
    "        if data['class'][i]=='spam':\n",
    "            sp_t[j][1]+=data['text'][i].count(sp[j])\n",
    "            \n",
    "       \n",
    "\n",
    "    \n",
    "hm_t "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31605"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12395"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(hm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer()\n",
    "counts = count_vectorizer.fit_transform(data['text'].values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtre BAISIEN MultinomialNB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Filtre BAISIEN MultinomialNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "classifier = MultinomialNB()\n",
    "targets = data['class'].values\n",
    "classifier.fit(counts, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['spam'], \n",
       "      dtype='<U4')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "examples = [\"Subject: christmas tree farm pictures\\n\"]\n",
    "example_counts = count_vectorizer.transform(examples)\n",
    "predictions = classifier.predict(example_counts)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('vectorizer',  CountVectorizer()),\n",
    "    ('classifier',  MultinomialNB()) ])\n",
    "\n",
    "pipeline.fit(data['text'].values, data['class'].values)\n",
    "p=pipeline.predict(examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['spam'], \n",
       "      dtype='<U4')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/toutou/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total emails classified: 5172\n",
      "Score: 0.96115983853\n",
      "Confusion matrix:\n",
      "[[3628   44]\n",
      " [  72 1428]]\n"
     ]
    }
   ],
   "source": [
    "k_fold = KFold(n=len(data), n_folds=6)\n",
    "scores = []\n",
    "confusion = numpy.array([[0, 0], [0, 0]])\n",
    "for train_indices, test_indices in k_fold:\n",
    "    train_text = data.iloc[train_indices]['text'].values\n",
    "    train_y = data.iloc[train_indices]['class'].values\n",
    "\n",
    "    test_text = data.iloc[test_indices]['text'].values\n",
    "    test_y = data.iloc[test_indices]['class'].values\n",
    "\n",
    "    pipeline.fit(train_text, train_y)\n",
    "    predictions = pipeline.predict(test_text)\n",
    "\n",
    "    confusion += confusion_matrix(test_y, predictions)\n",
    "    score = f1_score(test_y, predictions, pos_label=SPAM)\n",
    "    scores.append(score)\n",
    "\n",
    "print('Total emails classified:', len(data))\n",
    "print('Score:', sum(scores)/len(scores))\n",
    "print('Confusion matrix:')\n",
    "print(confusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtre BAISIEN BernoulliNB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BernoulliNB(alpha=1.0, binarize=0.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fILTRE BAISIEN BernoulliNB\n",
    "\n",
    "classifierb = BernoulliNB()\n",
    "targets = data['class'].values\n",
    "classifierb.fit(counts, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ham'], \n",
       "      dtype='<U4')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = classifierb.predict(example_counts)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipelineb = Pipeline([\n",
    "    ('vectorizer',  CountVectorizer()),\n",
    "    ('classifier',  BernoulliNB()) ])\n",
    "\n",
    "pipelineb.fit(data['text'].values, data['class'].values)\n",
    "pb=pipelineb.predict(examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ham'], \n",
       "      dtype='<U4')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total emails classified: 5172\n",
      "Score: 0.959691336923\n",
      "Confusion matrix:\n",
      "[[3627   45]\n",
      " [  75 1425]]\n"
     ]
    }
   ],
   "source": [
    "k_fold = KFold(n=len(data), n_folds=4)\n",
    "scores = []\n",
    "confusion = numpy.array([[0, 0], [0, 0]])\n",
    "for train_indices, test_indices in k_fold:\n",
    "    train_text = data.iloc[train_indices]['text'].values\n",
    "    train_y = data.iloc[train_indices]['class'].values\n",
    "\n",
    "    test_text = data.iloc[test_indices]['text'].values\n",
    "    test_y = data.iloc[test_indices]['class'].values\n",
    "    pipeline.fit(train_text, train_y)\n",
    "    predictions = pipeline.predict(test_text)\n",
    "\n",
    "    confusion += confusion_matrix(test_y, predictions)\n",
    "    score = f1_score(test_y, predictions, pos_label=SPAM)\n",
    "    scores.append(score)\n",
    "\n",
    "print('Total emails classified:', len(data))\n",
    "print('Score:', sum(scores)/len(scores))\n",
    "print('Confusion matrix:')\n",
    "print(confusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Filtre BAISIEN BernoulliNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=100, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape=None, degree=3, gamma='auto', kernel='linear',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifiersvm = SVC(kernel='linear', C=100)\n",
    "targets = data['class'].values\n",
    "classifiersvm.fit(counts, targets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ham'], dtype=object)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = classifiersvm.predict(example_counts)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipelinesvm = Pipeline([\n",
    "    ('vectorizer',  CountVectorizer()),\n",
    "    ('classifier',  SVC()) ])\n",
    "\n",
    "pipelinesvm.fit(data['text'].values, data['class'].values)\n",
    "psvm=pipelinesvm.predict(examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ham'], dtype=object)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "psvm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total  des emails classifiés: 5172\n",
      "Score: 0.14916197263\n",
      "Matrice de Confusion :\n",
      "[[3662   10]\n",
      " [1378  122]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"total  des emails classifiés: 5172\\nScore: 0.151798421428\\nMatrice de Confusion :\\n[[3665    7]\\n [1376  124]] pour C=100 ce qui montre que notre scores'ameliore avec l'augmentation de C bien comme la théorie l'annonce \""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k_fold = KFold(n=len(data), n_folds=6)\n",
    "scores = []\n",
    "confusion = numpy.array([[0, 0], [0, 0]])\n",
    "for train_indices, test_indices in k_fold:\n",
    "    train_text = data.iloc[train_indices]['text'].values\n",
    "    train_y = data.iloc[train_indices]['class'].values\n",
    "\n",
    "    test_text = data.iloc[test_indices]['text'].values\n",
    "    test_y = data.iloc[test_indices]['class'].values\n",
    "    pipelinesvm.fit(train_text, train_y)\n",
    "    predictions = pipelinesvm.predict(test_text)\n",
    "\n",
    "    confusion += confusion_matrix(test_y, predictions)\n",
    "    score = f1_score(test_y, predictions, pos_label=SPAM)\n",
    "    scores.append(score)\n",
    "\n",
    "print('total  des emails classifiés:', len(data))\n",
    "print('Score:', sum(scores)/len(scores))\n",
    "print('Matrice de Confusion :')\n",
    "print(confusion)\n",
    "\n",
    "\"\"\"Total des  emails classifiés: 5172\n",
    "Score: 0.145314516992\n",
    "matrice de Confusion :\n",
    "[[3665    7]\n",
    " [1381  119]] avec C =10 aussi pour C=1\"\"\"\n",
    "\n",
    "\"\"\"total  des emails classifiés: 5172\n",
    "Score: 0.14916197263\n",
    "Matrice de Confusion :\n",
    "[[3662   10]\n",
    " [1378  122]] pour C=100 ce qui montre que notre scores'ameliore avec l'augmentation de C bien comme la théorie l'annonce \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3665"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "les mots qui apparaissent le plus sur :\n",
      "Cluster 0:\n",
      " subject\n",
      " deal\n",
      " gas\n",
      " com\n",
      " meter\n",
      " 00\n",
      " 000\n",
      " thanks\n",
      " http\n",
      " know\n",
      " 2000\n",
      " mmbtu\n",
      " nomination\n",
      " need\n",
      " 2001\n",
      " new\n",
      " daren\n",
      " let\n",
      " enron\n",
      " day\n",
      " 01\n",
      " attached\n",
      " 10\n",
      " hpl\n",
      " th\n",
      " time\n",
      " www\n",
      " price\n",
      " want\n",
      " message\n",
      "Cluster 1:\n",
      " ect\n",
      " enron\n",
      " hou\n",
      " hpl\n",
      " 2000\n",
      " xls\n",
      " 000\n",
      " subject\n",
      " nom\n",
      " file\n",
      " teco\n",
      " 2001\n",
      " attached\n",
      " tap\n",
      " hplno\n",
      " cc\n",
      " pm\n",
      " actuals\n",
      " hplo\n",
      " corp\n",
      " gas\n",
      " 10\n",
      " 01\n",
      " meter\n",
      " deal\n",
      " 03\n",
      " forwarded\n",
      " 02\n",
      " daren\n",
      " 30\n",
      "\n",
      "\n",
      "\n",
      "['subject', 'deal', 'gas', 'com', 'meter', '00', '000', 'thanks', 'http', 'know', '2000', 'mmbtu', 'nomination', 'need', '2001', 'new', 'daren', 'let', 'enron', 'day', '01', 'attached', '10', 'hpl', 'th', 'time', 'www', 'price', 'want', 'message']\n",
      "\n",
      "\n",
      "\n",
      "['ect', 'enron', 'hou', 'hpl', '2000', 'xls', '000', 'subject', 'nom', 'file', 'teco', '2001', 'attached', 'tap', 'hplno', 'cc', 'pm', 'actuals', 'hplo', 'corp', 'gas', '10', '01', 'meter', 'deal', '03', 'forwarded', '02', 'daren', '30']\n"
     ]
    }
   ],
   "source": [
    "targets = data['class'].values\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "X = vectorizer.fit_transform(data['text'].values)\n",
    "\n",
    "true_k = 2\n",
    "classifiekm = KMeans(n_clusters=true_k, init='k-means++', max_iter=100, n_init=1)\n",
    "classifiekm.fit(X,targets)\n",
    "cl0=[]\n",
    "cl1=[]\n",
    "print(\"les mots qui apparaissent le plus sur :\")\n",
    "Top_Mots = classifiekm.cluster_centers_.argsort()[:, ::-1]\n",
    "Termes = vectorizer.get_feature_names()\n",
    "for i in range(true_k):\n",
    "    print(\"Cluster %d:\" % i),\n",
    "    for ind in Top_Mots[i, :30]:\n",
    "        print(' %s' % Termes[ind])\n",
    "        if i==0:\n",
    "            cl0.append(Termes[ind])\n",
    "        else :\n",
    "            cl1.append(Termes[ind])\n",
    "        \n",
    "\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "print(cl0)\n",
    "print(\"\\n\\n\")\n",
    "print(cl1)\n",
    "colors = ['b', 'g']\n",
    "markers = ['0', '', 's']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction d'un example:\n",
      "Cluster n°:\n",
      "[0]\n"
     ]
    }
   ],
   "source": [
    "print(\"Prediction d'un example:\")\n",
    "\n",
    "Y = vectorizer.transform([\"Subject: christmas tree farm pictures\\n\"])\n",
    "predictions = classifiekm.predict(Y)\n",
    "print(\"Cluster n°:\")\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0], dtype=int32)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipelinekm = Pipeline([\n",
    "    ('vectorizer',  CountVectorizer()),\n",
    "    ('classifier',  KMeans()) ])\n",
    "\n",
    "pipelinekm.fit(data['text'].values)\n",
    "pkm=pipelinekm.predict(examples)\n",
    "\n",
    "pkm"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
